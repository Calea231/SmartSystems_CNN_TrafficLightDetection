{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Smart_Systems_CNN_Traffic_Light_Detection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RCLkKkwEB2Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7d609de7-b1b6-4d07-f0e5-c2897df3ed28"
      },
      "source": [
        "\"\"\"Smart_Systems_CNN_Traffic_Light_Detection.ipynb: Chapter 7.\"\"\"\n",
        "\n",
        "__author__ = \"Marius Landmann\"\n",
        "__credits__ = \"Anna Dodik\"\n",
        "\n",
        "\n",
        "\"Other sources which helped writing the code: https://github.com/MariusLandmann/SmartSystems_CNN_TrafficLightDetection/blob/master/Sources/Links.docx\"\n",
        "\n",
        "# TensorFlow ≥2.0 is required\n",
        "%tensorflow_version 2.x\n",
        "# !pip install --upgrade deeplearning2020 \n",
        "# !pip install tensorflow_datasets\n",
        "# !pip install --upgrade tensorflow_datasets\n",
        "# !pip install tfds-nightly\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "assert tf.__version__ >= \"2.0\"\n",
        "\n",
        "#Benötigt die bereitgestellte GPU von Colab -> Test ob diese benutzt wird\n",
        "if not tf.config.list_physical_devices('GPU'):\n",
        "    print(\"No GPU was detected. CNNs can be very slow without a GPU.\")\n",
        "    if IS_COLAB:\n",
        "        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n",
        "\n",
        "\n",
        "##Importieren von benötigten Tools, Layertypen\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras.layers import Dense, Activation, Input, \\\n",
        "  Dropout, Conv2D, MaxPooling2D, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import reciprocal\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "!pip install absl-py\n",
        "from absl import app\n",
        "from absl import flags\n",
        "import os\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (0.9.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from absl-py) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXx5ELF0X3DZ",
        "colab_type": "text"
      },
      "source": [
        "# Cloning and Pulling the GitHub repository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ke4Omb3jyNeB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 918
        },
        "outputId": "afd9eddd-848e-4fa5-fc48-a29b36f1304e"
      },
      "source": [
        "# https://towardsdatascience.com/deeppicar-part-6-963334b2abe0\n",
        "# Forked repository\n",
        "repo_url = 'https://github.com/MariusLandmann/SmartSystems_CNN_TrafficLightDetection'\n",
        "\n",
        "#Clone repository\n",
        "%cd /content\n",
        "\n",
        "repo_dir_path = os.path.abspath(os.path.join('.', os.path.basename(repo_url)))\n",
        "\n",
        "!git clone {repo_url}\n",
        "%cd {repo_dir_path}\n",
        "\n",
        "print('Pull it')\n",
        "!git pull\n",
        "\n",
        "\n",
        "##prepare training data --> from deeppicar\n",
        "#installing packages for it\n",
        "%cd /content\n",
        "!git clone --quiet https://github.com/tensorflow/models.git\n",
        "\n",
        "!apt-get install -qq protobuf-compiler python-pil python-lxml python-tk\n",
        "\n",
        "!pip install -q Cython contextlib2 pillow lxml matplotlib\n",
        "\n",
        "!pip install -q pycocotools\n",
        "\n",
        "%cd /content/models/research\n",
        "!protoc object_detection/protos/*.proto --python_out=.\n",
        "\n",
        "os.environ['PYTHONPATH'] += ':/content/models/research/:/content/models/research/slim/'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Cloning into 'SmartSystems_CNN_TrafficLightDetection'...\n",
            "remote: Enumerating objects: 8, done.\u001b[K\n",
            "remote: Counting objects: 100% (8/8), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 13921 (delta 1), reused 8 (delta 1), pack-reused 13913\u001b[K\n",
            "Receiving objects: 100% (13921/13921), 515.94 MiB | 26.69 MiB/s, done.\n",
            "Resolving deltas: 100% (302/302), done.\n",
            "Checking out files: 100% (13532/13532), done.\n",
            "/content/SmartSystems_CNN_TrafficLightDetection\n",
            "Pull it\n",
            "Already up to date.\n",
            "/content\n",
            "Selecting previously unselected package python-bs4.\n",
            "(Reading database ... 144379 files and directories currently installed.)\n",
            "Preparing to unpack .../0-python-bs4_4.6.0-1_all.deb ...\n",
            "Unpacking python-bs4 (4.6.0-1) ...\n",
            "Selecting previously unselected package python-pkg-resources.\n",
            "Preparing to unpack .../1-python-pkg-resources_39.0.1-2_all.deb ...\n",
            "Unpacking python-pkg-resources (39.0.1-2) ...\n",
            "Selecting previously unselected package python-chardet.\n",
            "Preparing to unpack .../2-python-chardet_3.0.4-1_all.deb ...\n",
            "Unpacking python-chardet (3.0.4-1) ...\n",
            "Selecting previously unselected package python-six.\n",
            "Preparing to unpack .../3-python-six_1.11.0-2_all.deb ...\n",
            "Unpacking python-six (1.11.0-2) ...\n",
            "Selecting previously unselected package python-webencodings.\n",
            "Preparing to unpack .../4-python-webencodings_0.5-2_all.deb ...\n",
            "Unpacking python-webencodings (0.5-2) ...\n",
            "Selecting previously unselected package python-html5lib.\n",
            "Preparing to unpack .../5-python-html5lib_0.999999999-1_all.deb ...\n",
            "Unpacking python-html5lib (0.999999999-1) ...\n",
            "Selecting previously unselected package python-lxml:amd64.\n",
            "Preparing to unpack .../6-python-lxml_4.2.1-1ubuntu0.1_amd64.deb ...\n",
            "Unpacking python-lxml:amd64 (4.2.1-1ubuntu0.1) ...\n",
            "Selecting previously unselected package python-olefile.\n",
            "Preparing to unpack .../7-python-olefile_0.45.1-1_all.deb ...\n",
            "Unpacking python-olefile (0.45.1-1) ...\n",
            "Selecting previously unselected package python-pil:amd64.\n",
            "Preparing to unpack .../8-python-pil_5.1.0-1ubuntu0.2_amd64.deb ...\n",
            "Unpacking python-pil:amd64 (5.1.0-1ubuntu0.2) ...\n",
            "Setting up python-pkg-resources (39.0.1-2) ...\n",
            "Setting up python-six (1.11.0-2) ...\n",
            "Setting up python-bs4 (4.6.0-1) ...\n",
            "Setting up python-lxml:amd64 (4.2.1-1ubuntu0.1) ...\n",
            "Setting up python-olefile (0.45.1-1) ...\n",
            "Setting up python-pil:amd64 (5.1.0-1ubuntu0.2) ...\n",
            "Setting up python-webencodings (0.5-2) ...\n",
            "Setting up python-chardet (3.0.4-1) ...\n",
            "Setting up python-html5lib (0.999999999-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "/content/models/research\n",
            "object_detection/protos/input_reader.proto: warning: Import object_detection/protos/image_resizer.proto but not used.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTUQd_SBYXN6",
        "colab_type": "text"
      },
      "source": [
        "# Loading the Dataset\n",
        "& convert the xml files to a single csv file\n",
        "\n",
        "& generate the TFRecord files\n",
        "\n",
        "**Wichtig:** Die zwei Codes aus meinem GitHub benutzen! Eine lange Fehlersuche könnte ansonsten die Folge sein. generate_tfrecord.py findet man zwar online auf anderen Seiten allerdings sind diese auf Tensorflow 1.xx ausgelegt. Ich musste den Code teilweise umschreiben, damit er mit Tensorflow 2.xx verwendet werden kann"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPKYmpI-y2L6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "0464c665-6eb7-48a7-b1a1-9d8c45adc4d3"
      },
      "source": [
        "#(https://towardsdatascience.com/deeppicar-part-6-963334b2abe0)\n",
        "repo_dir_path = '/content/SmartSystems_CNN_TrafficLightDetection'\n",
        "%cd {repo_dir_path}/traffic_light_detection\n",
        "\n",
        "\n",
        "\n",
        "# Convert train folder annotation xml files to a single csv file,\n",
        "# generate the `label_map.pbtxt` file to `data/` directory as well.\n",
        "!python code/xml_to_csv.py -i data/train -o data/annotations/train_labels.csv -l data/annotations\n",
        "\n",
        "# Convert test folder annotation xml files to a single csv.\n",
        "!python code/xml_to_csv.py -i data/test -o data/annotations/test_labels.csv\n",
        "\n",
        "\n",
        "\n",
        "# Generate `train.record`\n",
        "!python code/generate_tfrecord.py --csv_input=data/annotations/train_labels.csv --output_path=data/annotations/train.record --img_path=data/train --label_map data/annotations/label_map.pbtxt\n",
        "\n",
        "# Generate `test.record`\n",
        "!python code/generate_tfrecord.py --csv_input=data/annotations/test_labels.csv --output_path=data/annotations/test.record --img_path=data/test --label_map data/annotations/label_map.pbtxt\n",
        "\n",
        "\n",
        "test_record_fname = repo_dir_path + '/traffic_light_detection/data/annotations/test.record'\n",
        "train_record_fname = repo_dir_path + '/traffic_light_detection/data/annotations/train.record'\n",
        "label_map_pbtxt_fname = repo_dir_path + '/traffic_light_detection/data/annotations/label_map.pbtxt'\n",
        "\n",
        "\n",
        "print(type(test_record_fname))\n",
        "print(len(test_record_fname))\n",
        "print(test_record_fname)\n",
        "#!cat data/annotations/train_labels.csv\n",
        "#!cat {label_map_pbtxt_fname}\n",
        "#!cat {train_record_fname}"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/SmartSystems_CNN_TrafficLightDetection/traffic_light_detection\n",
            "Successfully converted xml to csv.\n",
            "Generate `data/annotations/label_map.pbtxt`\n",
            "Successfully converted xml to csv.\n",
            "2020-07-08 02:46:16.163344: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "Successfully created the TFRecords: /content/SmartSystems_CNN_TrafficLightDetection/traffic_light_detection/data/annotations/train.record\n",
            "2020-07-08 02:46:20.891989: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "Successfully created the TFRecords: /content/SmartSystems_CNN_TrafficLightDetection/traffic_light_detection/data/annotations/test.record\n",
            "<class 'str'>\n",
            "100\n",
            "/content/SmartSystems_CNN_TrafficLightDetection/traffic_light_detection/data/annotations/test.record\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leWZYKHCOiQN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "9ff2d5ff-0c58-4147-8092-2ce90d4ccfd3"
      },
      "source": [
        "#Recorddatei öffnen\n",
        "file_testrecord=open(train_record_fname,'rb')\n",
        "print(file_testrecord.readlines(2000))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[b'\\xe80\\x07\\x00\\x00\\x00\\x00\\x00\\x8e\\x7f\\xd6\\xb8\\n', b'\\xe4\\xe1\\x1c\\n', b'\"\\n', b'\\x16image/object/bbox/ymin\\x12\\x08\\x12\\x06\\n', b'\\x0433\\xc3>\\n', b'(\\n', b'\\x0fimage/source_id\\x12\\x15\\n', b'\\x13\\n', b'\\x11Picture 1 (1).png\\n', b'\\xe1\\xde\\x1c\\n', b'\\rimage/encoded\\x12\\xce\\xde\\x1c\\n', b'\\xca\\xde\\x1c\\n', b'\\xc6\\xde\\x1c\\x89PNG\\r\\n', b'\\x1a\\n', b'\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x02\\x80\\x00\\x00\\x01\\xe0\\x08\\x02\\x00\\x00\\x00\\xba\\xb3K\\xb3\\x00\\x00 \\x00IDATx\\x01\\x84\\xc1\\xdb\\xafm\\xe9y&\\xf4\\xe7y\\xdf\\xef\\x1bc\\x1e\\xd6Z\\xfbT{\\x97\\xab\\\\>\\xa5\\xec\\xd8\\xce\\xd1\\xb1\\x1dw\\xd2M\\xa7\\xd3\\xee\\x84\\x8e\\xd4D\\x88\\xb4C\\xb8\\x01\\x92\\x10\\x90\\x9a+\\xb8\\x80\\x16\\x12H\\x91\\x10\\x88?\\x81\\x9b\\xe6\\x82\\x88\\x93\\x14\\xd4B\\t-\\xa1\\xc4\\xd0i\\x92\\x8e\\t\\x8eBw\\xda\\xa9\\x8a\\x1d;>\\x96\\x0fu\\xd8{\\xaf\\xb5\\xe6\\x9cc\\x8c\\xef{\\xdf\\x871+\\x87\\x02\\x0b\\xd2\\xbf\\x1f\\x7f\\xe8_\\xfd\\xc1\\xc8\\xfc\\xec\\xa7?3\\xcfs\\xde\\\\\\x99\\xd9\\xb6\\x0e\\xc5\\xfd\\xc1\\xc5\\xbd\\xddn\\xf7\\xe2\\xe5\\xdf\\xfa\\xf8\\xc7?\\xfeS\\x7f\\xe7\\xa3\\x8f\\x9e\\xddfI\\x00\\x06\\xc3\\x9f\\t\\x9c%\\x12gvs8\\xc4\\xf5\\x0e\\xc0\\xdd\\x07\\xac\\x05a\\t a\\x00\\x02\\x0b\\x80\\x1b\\xc4W\\xbf\\xf1\\xd5\\'_\\xbf\\xf9\\xd4\\xa7\\xfe\\x8f\\xd7\\xbf\\xfa\\xf8p8>z\\xf6\\xfe\\xe5\\xe5%M+\\x8f\\x02`\\xc9\\xde#\\x94\\xd6Z\\xdf\"%-\\xe8\\xca\\xbc)\\xd9{\\x0c\\xed\\r7k\\xb9\\x9f\\xa6i@!\\r@\\x8f~\\xd0IR\\xd1M\\xef1\\xd4\\x87\\xc7\\xd3\\xb1\\x9eF\\xa5,\\xea<\\xcf\\xd3\\xf6\\x98\\x99q|\\xba\\xdb\\xef\\x1bo\\x00\\xd0\\xf0\\xf4\\xc9\\x93\\x07w\\x1e\\xbc\\xfa\\xea\\xab\\xf7\\x1e\\xbc{\\x9egW\\xcf\\x0cjG\\xe3\\xa6,\\xad\\xf7\\xd2\\xd1Z\\xd7\\xe0$\\xe5[\\xae\\x06\\x8b\\x88}\\xecS\\x99eq7\\xd9\\xa6\\xf5\\x9e\\xaey\\x9e/\\xcav\\x9a\\xa6\\x8b\\x8b\\xd1K\\x19\\x17\\x972\\x99\\xa4\\xcd\\x95\\x00\\xe6\\xe5TK\\t\\xb6\\xe2e\\x08H\\t\\x96\\xc8\\x18\\xa6\\x07\\x00\\x96\\xf2\\x18\\x80m\\xe1f\\xcb\\xb2HZ\\xaa\\xdc|\\xbf\\x98\\x94\\xcb\\xd2\"#\\x87\\x01\\xc0\\xd5\\x88\\xe3\\xe1\\xc0rO\\xd2\\xae/\\x00\\x0e^#s+dfc\\x91\\xb2\\xd5\\xa4\\xb16E\\xf49C\\xcab7\\x92\\xc6v\\x95\\x19\\x07\\xa7\\x91\\x96\\xcaTi\\x1dg\\x867eF\\x1a\\xcd}6\\x01\\x98\\xdaRK\\xd9\\x87\\x01\\x98\\xeb\\xa1\\xd6\\xea\\xaa\\x99\\x19u\\x07`Z^\\x1f\\xc7\\xd1\\xb8w\\xf7\\x8b\\x86U\\x94\\x91d3\\xb8\\x9b\\xd5\\x06 \\x06\\xaej\\x1f\\xccM\\xe9\\xb5\\xd4\\x121\\x8e\\xa3\\xab\\x03\\x988\\xba\\xadf7+\\x1c\\x01\\x88\\x15\\xabr\\xf2\\xe2\\x9b\\xbe\\xa1\\x11\\\\Hvw#G8\\xfe\\x9c\\x17#O\\xcb\\x1b\\xf3<o\\xb1]\\x96\\xf6\\x9d\\xef{a\\x1c\\xc7\\x8b\\xbb\\xe3v\\xbb\\xbb|\\xf6\\xea\\xc1\\x83g\\xaa\\xed\\x86a\\xb0\\xba+\\xb5\\xa6\\x98\\x12\\xe6A\\xa9\\x9c\\xd5{/\\x96\\x00\\xdc\\x1aVV\\xdd\\x8d5\\xdc\\x9ct/^0\\x9a\\xd1\\xcch\\x86\\x15qF\\xfc\\xff\"\\xbe\\x9d\\xf0\\x16\\xc7\\x99p&\\x9c5\\xa8\\x83D\\x06X\\x1aI\\xd1\\xe8\\x06\\xe1-\\x1dp\\xc0\\x16\\x94\\x02K\\x00\\x81\\x02\\xc0\\xf1\\xcf\\x978\\x13\\xce\\xbcc>\\x02B\\xa4Z\\xb6\\xe8\\x9d\\xb9{\\xfa\\xe4\\xe9\\xed\\xe3\\x9b\\xa7O\\x9f>9\\xdc\\\\__\\xf7\\xe9p<\\x1eS\\x87\\xe3\\xe94iY\\x96\\xd6\\xa6\\x19\\x80S^\\\\\\xbbm\\xadu;\\xf4q\\x1c\\xb7\\xbe3\\xb7\\xca\\xad\\xb9\\x0b\\x90\\xd4\\xa7y\\xf5\\xecs\\x97\\xdb\\xed\\xb6\\x8cw\\xee\\xdd\\xbb\\x0b\\xab\\xc30\\xb8@R0\\x00\\x01a%\\x03P\\x18X\\xc9pV\\x8c\\xac\\xae\\x15\\x00\\x92\\x89\\x81\\xb4\\xf0\\x85\\xa4+\\x8d\\xec4#\\x8d\\x02\\xd0\\xbd\\xa74\\xa4\\x9b\\xd1\\xd3\\x94\\n', b\"h\\xd5-\\x010(\\x89$\\x00\\xc9\\x95I$i`\\x02P\\x14I!f&\\xd8%\\x05:\\x80\\x12\\r@\\xf3\\x04`!\\x92\\t3#1e\\xe6\\x12\\x1bI\\x86\\x8c\\x88\\xae.\\xc9B\\x11\\xbdwf\\xc4I\\xd3\\xb2,\\xf3\\xc1N\\xa7\\xe3rs\\x9c\\xa6\\xe9d\\xa7\\x8c\\xecdD\\x0eK\\x03P\\xa03[H6\\x8d\\x92\\xc2\\x97\\xa1\\x0e[\\xbf\\xb8\\xb8\\xd8\\x8f\\xfb;4\\x1a\\x077\\xc70\\x01(\\xcdIN\\xa5\\x98\\xd9\\x16!\\x89t\\x00\\x82aU\\xd2\\xdd\\x00\\x90\\xf46\\xbay\\xb2\\x03\\xa8\\xea\\x00\\xd2\\t\\xc0\\xb2\\xb4\\xd6\\x9e\\xce\\xb7\\xa7i\\xb2i\\xceL\\x95\\x83\\x91\\xcc\\r\\xc9`\\xe1\\xcagIU\\x02\\xb0\\xd0\\xdd\\x0b`\\x92\\xe0\\r\\xc0\\x18\\x95\\xa4\\x08\\x92\\xce\\xa0q.\\x03\\x80\\x12\\x1d\\x808Hi:\\x01\\xb0\\xd8\\x999\\xe9\\x00D\\xac\\x88\\xc5\\xcc\\xe0a\\xee\\xca\\x0b\\x92\\x92\\x00\\xc8\\xa3\\xb8'\\xcd\\xcd\\x9b\\x19\\x80M7\\x00\\x14\\xce\\x8c8K3K\\xafJ\\x95\\x9c%-T*\\xa3v\\x00Clq\\x96\\x00\\x88.\\xc9d\\x00\\x88\\x0e\\xe0\\xf5\\xe9\\x9b?\\xfa\\xa3?\\xba\\xdb\\xbf\\x03\\xc0\\xe9\\xc9\\xad\\xbb?\\xd8\\xf2\\x95W^\\xf9\\xa3\\xff\\xf6\\x93_\\xfe\\xf2\\x97_<\\xcc\\x97\\x97\\x97\\xf8\\xee\\xfa\\xbd?\\xfc\\xc3O\\xde\\xff\\xb1y\\x9e\\x9f>\\x99\\x8f\\xc7\\xe3\\x1f\\xbf\\xfc{w\\xee\\xdcy\\xff\\x87?d\\xe6\\xb5\\x0e\\xa4!\\x8c\\xb4?\\xfc\\xdck$\\x9f\\x7fG\\x1d\\x86A\\xe6x\\x93R]\\x88\\xde\\x9d\\x01\\xa0\\xe4\\x85\\x94\\x9e\\xb6,\\xed\\x7f\\xfe\\x1f\\xfe\\xf1\\xaf\\xfc\\xca\\xaf<\\xf7\\xb1\\xe3\\xcf\\xff\\xfc\\xcf}\\xe0\\x9d\\x1f\\xba\\xbd\\xbdu+4R\\xd0\\n\", b'\\x88\\xcc\\x90K\\xb2\\xd9\\x9e<}\\x1aE\\x00\\xf8\\xbe\\xbf\\xfa\\xbe\\xeb\\xeb\\xebo\\xbe\\xfc\\xcdqk\\xd9\\xb6u\\x18.\\xed\\xc2\\xccvw\\x9f\\xbfs\\xe7\\xce\\xdf\\xfc\\x9e\\x9f\\xff\\xc4\\xdf\\xfe\\xc4\\xf7\\xfeK\\x04\\xe0x\\x8bp\\x168[\\x16\\xdc\\xdc\\xdc\\xfa2D\\xe4\\xb3/l\\x00\\x04\\x12g\\t\\xe0\\x84\\x9e\\xd0\\xd7\\xbe\\xf9\\xb5\\xaf|\\xe5\\xcb_\\xfe\\xe2\\x1b/\\xbf\\xfc\\x87\\xc7\\xd7\\xe7\\xddn\\xf7\\xdc\\xf3\\x0f\\xcd\\x8c\\xbdK\\xd9\\xa8\\x95\\'\\xa2\\xc7)-3\\xab\\xa6\\xd6\\x1a`\\x922\\x97\\xc8\\xe8\\xf0\\xe8\\x01\\\\\\x03\\xe8y\\xd9#0\\xcf\\xc5Kdd\\xe6\\xab\\xb7\\x9f-^6\\x05\\x91\\x19\\xba\\xa8\\xb5\\xea\\xb5\\xabi:\\x8dC\\x92\\x9c\\xf3\\x98R\\x8b7\\xeaP\\xc3\\x86\\xcc(v/\"\\xc6\\xea\\xd7\\xd7\\xd7\\xe3n$)Ed\\xdc\\xed\\xf7J\\xf1\\x1c{\\xefmh\\x83\\xb9c\\x93\\x92nZx)\\xc3\\x96\\x00\\x06\\x1f\\xe6y\\xb6\\x8cq\\x1c-KdV\\x08@\\x19\\xcb4Mi\\xf4\\xe2%G\\x00\\x19ER\\xad\\x8a\\x8c\\xaet\\xb3,s\\xeb\\xbd\\xe4\\xd6\\xcdf\\xcd$\\x07]\\x00\\xa0]\\x93\\xd4\\xb8_\\x96\\xe5\\xb4\\x9c\\xdc\\xbclDr\\x98\\x0b\\x80\\x9e7\\xee\\x96Q\\x01D\\x11\\x80\\xd6+\\x80]43\\x1eM\\x006\\xdd\\x00\\x9c\\x94\\x00h\\x01\\xa0D\\xc9\\xccc\\xb9\\x96r\\xf0Q\\x12N\\x0e`\\xa1\\x03P\\xc7j\\xec-z\\xa7)r5\\x92\\xecE\\xab@\\x03\\xd0\\xd3\\xe6y\\xbe\\xf2mdh\\xb8\\x05P\\x84\\xd5d[\\x00\\xe3b4\\xeb%$\\xddI\\x018\\x8e4w3\\x030\\xa0\\x98y\\x96\\x1b\\x00\\xd2F\\x12\\xca\\xce\\xcc\\xca\\xb1\\xdc\\xbd{\\x97\\xf9\\x14@\\xd3\\xd6\\xcdc\\x13fV\\x8b)e*$\\x85S\\xa9e\\xeb\\xbb\\x88P]\\xcc\\xcdr\\x00@\\x0c\\x00\\xf8\\'\\x06\\xd1\\xcc<\\x01X\\x16\\x92\\xba\\x9e{\\xf4\\xcd\\x85v\\xfb\\xfd\\xf6\\xa2\\x8c\\xe3x\\xef\\xce\\xf6\\xde\\xfd\\xfb\\x9b\\xfb\\x0f\\xee\\xde\\xbb[\\xb7[\\x1a\\xef\\x8e\\x0f\\xfa\\xd9\\x06@\\x86g\\x8a\\xa7E\\x92\\x95N\\x1a<\\x00\\x98\\x153s\\x17I\\x1f\\x8c\\xb4\\x01\\x1b3\\x03\\t\\x02\\x86\\x7f>\\xe1-\\xc4[\\x843\\xe2\\xccp\\xd6\\x81\\x04\\x0c\\xd9\\x00u\\x00t\\xa3\\x1bV\\x06$\\xceBp\\xc2\\x02dz\\x07 \\x0c\\x00\\x883\\xc3_$q&\\x9c\\xf9\\x84e\\x86\\x94\\xad\\xf5\\x86\\x96\\x99\\xd3\\x92777\\xd7\\xd7\\xb8\\xbe\\xbe\\x9e\\xaf\\xafonn\\xda\\xf4\\xc6\\xed\\xed\\xcd\\x14\\xd3\\xd2\\x96>\\xf5\\xd6[DKI\\xdb4\\xf3\\x8b\\x8bm\\xa9u[\\x86a\\xa8\\x83\\x8f\\xa5\\x14\\xda&S\\xd9{fz\\x1c\\x01\\x8c\\xdb\\xfd\\x0b/\\xbc\\xb0\\xd9\\xdf\\xddl\\xb6\\x94\\xb9Y\\x12\\x92\\xc2;Wi\\x92\\x0613\\x89\\n', b' inF\\x0b\\x00\\xa6\\xa41s\\x8fU9\\x91\\xc4\\x9bd4c\\x81\\x00\\xb4\\xe2J\\x15\\x90\\xb4\\x12\\x8b\\xa4\\xce\"\\xc9\\xd0\\x01$,3!\\xa3\\x11@FRFR\\x08\\x00\\xc2Y\\xf413\\xc1ER\\xa0IiA\\x00\\x89\\x82\\x15\\'\\xfc\\x19\\x03H\\xce\\xd9tFe*r\\x85d\\xf4\\xae6d\\xe6\\xa4\\x9b\\xde\\xfb\\xf5\\xf1\\xd4Z[\\x8eO\\x0f\\xc7\\xc3\\xdc32V\\x00l\\x11\\x00b\\x93\\x19\\xadL$\\x0b\\x16\\x00$\\xa4\\xdc\\xda\\xdd\\xab\\xab\\xabz\\xe7\\xca\\xcc\\xe0r/$\\x01X\\x10\\xc0RF/e\\xa3\\x19\\x80\\xa7\\x01Hb\\xe5\\x0e3k\\x85\\x006mK\\x12\\xec\\x00\\x1c\\x13Wp\\x1aS\\xec\\xbd\\xdf,\\xfdp{@\\xb4\\xe8\\x812\\x03\\xf0(4\\x1a+\\x80,\\x01\\xa0f\\x07p\"\\x01\\xb8G\\x8f.v3\\xdb\\xf6\\x8d\\xb9A\\x83\\x91\\xb4\\x99\\xc6^@\\xb24\\x9721\\x02Xl\\x010,#W\\xd6h\\x867I\\x0bi\\xeeU\\x124\\x90tw\\x00S\\t\\xd2H\\xaf\\xb5(\\t\\xc0\\xb3\\x00p6\\x00\\xbd`\\xe5\\xda\\x93\\x04\\x17IP\\xc3Y\\x07 \\x0c\\x00,\\xf7X\\xf9\\xb5\\xa4\\xaa\\x9aRX\\xc7Y\\x02\\xf8\\xc6\\xe3\\xaf|\\xf4\\xa3\\x1f}\\xf1\\xc5\\xef\\xeb=\\xe6k\\xce\\xf3<z\\x02\\xf8\\xcc\\xaf|\\xfa\\x93\\xff\\xeb\\'\\xeb\\xd7?\\xf7\\xa1\\x0f}\\xe8m\\xdf\\xbb\\xf9\\xe0w}\\xd7\\xf4\\xfe\\xbfB\\xf27\\xff\\xd1\\xa7ooo\\x7f\\xef\\x1f\\xff\\xc3{\\xf7\\xef\\xff\\xc2\\x7f\\xf8\\x0b=b\\xcc\\x0b3\\x07\\x10\\xbd\\xdf\\xb6Mk}\\xbfM7#\\x9dd $E\\xe7\\xe1p\\xf0:\\xd5:TU37\\xe4\\xfeb\\xff\\xd5\\xcf\\xdc\\xfe\\xfa\\'\\x7f\\xfdW~\\xf5\\xef\\x7f\\xec\\x07?\\xf6s\\xff\\xd1\\xdfNi\\x88\\xad\\xbb\\x01P*\\x9d)1\\xa9\\x14U\\xa2\\xf7\\xe3\\xd3%z\\xe7\\xbb~\\xe0\\xbd_\\xff\\xfa\\xd7\\xe3\\xd5\\xc3v\\x8bS\\xeev\\xbb\\xede<\\xbf\\xdb\\xed\\xbam?\\xf6\\x83\\x1f\\xfb\\x89\\xbf\\xf4\\x93?\\xfb\\xb3\\xffB\\x7f\\x84\\x95\\xe3L8\\x0b\\x9c\\x9d\\x1a\\x0e\\xc7\\xd3p=\\xb4\\xd6\\x1e}\\xc7\\x06@\\xe2\\xac\\xa1\\x03\\xe8\\xe8O\\x96\\xa7_\\xf9\\xd2\\xab_\\x7f\\xe5\\x95\\xcf\\xfc\\xe6\\xef\\x1f\\x0f\\x87\\xd3\\xa6<\\xfb\\xec\\xdb\\x8a\\xf7\\xe8\\x81\\xb0\\x88\\xa8\\xb3I9\\xe1@\\xd2R\\xd1\\xfbM(#\\x062\\x95=\\x11\\x11\\xe8S\\xefQ\\x0c\\x99\\xa9\\xcc\\x1e\\xd1sO2\\xfa!3\\xcdl\\x18\\x86\\xc3\\xe3\\xcf\\xa5\\x12\\xb99\\x1c\\x0e\\xcbi\\xa8\\xb5n\\xeb#\\xad\\xecu\\x00\\xc4-\\xc9C;\\xed\\xb6\\xbb\\x86+\\x92uy\\xc6\\xcc\\x8b\\xf7\\xe2>\\xfb\\x93\\xc8\\xcc\\xa5\\xb8\\xd9\\xd6\\xb6\\\\\\r\\xe8\\xbd\\x8f\\xcbT\\x8a\\x9f6,\\xc51dkm\\x1bW\\x92\\xe6\\xb2H\\xdaa \\xe9\\xeaR*\\xbd\\x94r\\xa8\\xb3\\x99a\\xae\\x00\\xc6t\\x00F\\x07@%\\x80\\xa9.\\xe6&&I\\x8f\\xc9\\xddO\\xc7y\\xb3\\xd9\\xa8\\xed$u\\xc8\\xdc\\xd4\\xb7\\x91)\\xbf\\xe5j\\xbc&Y\\x96\\xcbL1])o\\x03i\\xc7\\xed\\x0c u\\xc8\\xcca\\xb9\"\\x89z\\x03\\xc0\\xbb\\x8c\\\\\\xac\\xf5\\x88\\x91\\x9d\\xa4i\\xdf#N~c\\xb4\\xd6\\x0b\\x80\\x8e\\x81\\xe4\\xd8\\xb4\\xca\\xb6\\x914\\x94\\xd6[\\x03\\xbb$\\xc8R\\x9a;I\\x9b\\xea\\x13\\x92\\xd0\\x16\\x80\\xb7 \\xb9\\xe4\\xad\\x99\\xb9FIs\\n']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEjpOHl6HHCm",
        "colab_type": "text"
      },
      "source": [
        "# Ansatz X\n",
        "Standart CNN Architektur gewählt, mit dem Ziel unser CNN mit unserem Datensatz im TFRecordformat zu trainieren.\n",
        "\n",
        "Das Problem: kann bis jetzt nicht mit dem TFRecordformat trainiert werden, obwohl das Netz selber mit anderen Datensätzen (keine TFRecord files) funtionsfähig ist\n",
        "\n",
        "Vermutung: Man braucht andere Funktionen wenn man den TFRecorddatensatz benutzen will"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-qoj7lqHHqq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###Architektur\n",
        "\n",
        "\n",
        "\n",
        "# model\n",
        "batch_size = 32\n",
        "learning_rate=0.001\n",
        "momentum=0.9\n",
        "dense_neurons=300\n",
        "n_filters=300\n",
        "first_kernel_size=(7,7)\n",
        "\n",
        "activation='elu'\n",
        "\n",
        "# input size of images with RGB color\n",
        "input_layer = Input(shape=(640, 480, 3))\n",
        "\n",
        "# Convolutional Neural Network\n",
        "# It consists of 5 stacked Convolutional Layers with Max Pooling\n",
        "model = Conv2D(\n",
        "    filters=256,\n",
        "    kernel_size=(7,7),\n",
        "    activation=activation\n",
        ")(input_layer)\n",
        "model = MaxPooling2D((2,2))(model)\n",
        "\n",
        "model = Conv2D(\n",
        "    filters = 256, \n",
        "    kernel_size=(3,3), \n",
        "    activation=activation\n",
        ")(model)\n",
        "model = MaxPooling2D((2,2))(model)\n",
        "\n",
        "model = Conv2D(\n",
        "    filters = n_filters, \n",
        "    kernel_size=(3,3), \n",
        "    activation=activation\n",
        ")(model)\n",
        "model = MaxPooling2D((2,2))(model)\n",
        "\n",
        "model = Conv2D(\n",
        "    filters = n_filters, \n",
        "    kernel_size=(3,3), \n",
        "    activation=activation\n",
        ")(model)\n",
        "model = MaxPooling2D((2,2))(model)\n",
        "\n",
        "model = Conv2D(filters = n_filters, \n",
        "  kernel_size=(3,3), \n",
        "  activation=activation, \n",
        "  padding='same'\n",
        ")(model)\n",
        "model = MaxPooling2D((2,2))(model)\n",
        "\n",
        "model = Conv2D(filters = n_filters, \n",
        "  kernel_size=(3,3), \n",
        "  activation=activation, \n",
        "  padding='same'\n",
        ")(model)\n",
        "model = MaxPooling2D((2,2))(model)\n",
        "\n",
        "model = Conv2D(filters = n_filters, \n",
        "  kernel_size=(3,3), \n",
        "  activation=activation, \n",
        "  padding='same'\n",
        ")(model)\n",
        "\n",
        "# Fully-Connected-Classifier\n",
        "model = Flatten()(model)\n",
        "model = Dense(\n",
        "    dense_neurons,\n",
        "    activation=activation\n",
        ")(model)\n",
        "\n",
        "model = Dense(\n",
        "    dense_neurons / 2,\n",
        "    activation='tanh'\n",
        ")(model)\n",
        "\n",
        "# Output Layer\n",
        "output = Dense(10, activation=\"softmax\")(model)\n",
        "\n",
        "CNN_model = Model(input_layer, output)\n",
        "\n",
        "# Compiling model\n",
        "optimizer = keras.optimizers.SGD(lr=learning_rate, momentum=momentum)\n",
        "CNN_model.compile(\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    optimizer=optimizer,\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "CNN_model.summary()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#ALLES AUSKLAMMERN: markieren -> strg+shift+7\n",
        "\n",
        "#!cat {test_record_fname}\n",
        "#!cat {train_record_fname}\n",
        "\n",
        "# Train the model\n",
        "history2 = CNN_model.fit(\n",
        "    train_record_fname,\n",
        "    epochs=1,\n",
        "    validation_data= test_record_fname\n",
        "    \n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# model.fit(\n",
        "#         x=train_record_fname,\n",
        "#         #steps_per_epoch=1281167 // batch_size,\n",
        "#         epochs=1\n",
        "#         validation_data=test_record_fname,\n",
        "#         #validation_steps=50000 // batch_size,\n",
        "#         #callbacks=[learning_rate, model_ckpt, tensorboard],\n",
        "#         # The following doesn't seem to help in terms of speed.\n",
        "#         # use_multiprocessing=True, workers=4,\n",
        "#         #epochs=epochs\n",
        "#         )\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtAgPHGoOcxQ",
        "colab_type": "text"
      },
      "source": [
        "#Eine komplexere Archetektur, welche normalerweise funktionieren würde:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZ3PWrMFOloa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tf.keras.Sequential(\n",
        "#     layers=None, name=None\n",
        "# )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model = keras.Sequential()\n",
        "model.add(Conv2D(64,(3,3),activation='relu',input_shape=(640,480,3), padding='same'))\n",
        "model.add(Conv2D(64,(3,3),activation='relu',padding='same'))\n",
        "model.add(Conv2D(64,(3,3),activation='relu',padding='same'))\n",
        "model.add(MaxPooling2D((2,2),strides=(2,2)))\n",
        "\n",
        "model.add(Conv2D(128,(3,3),activation='relu',padding='same'))\n",
        "model.add(Conv2D(128,(3,3),activation='relu',padding='same'))\n",
        "model.add(MaxPooling2D((2,2),strides=(2,2)))\n",
        "\n",
        "model.add(Conv2D(256,(3,3),activation='relu',padding='same'))\n",
        "model.add(Conv2D(256,(3,3),activation='relu',padding='same'))\n",
        "model.add(Conv2D(256,(3,3),activation='relu',padding='same'))\n",
        "model.add(MaxPooling2D((2,2),strides=(2,2)))\n",
        "\n",
        "model.add(Conv2D(512,(3,3),activation='relu',padding='same'))\n",
        "model.add(Conv2D(512,(3,3),activation='relu',padding='same'))\n",
        "model.add(Conv2D(512,(3,3),activation='relu',padding='same'))\n",
        "model.add(MaxPooling2D((2,2),strides=(2,2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(600,activation='relu'))\n",
        "model.add(Dense(600,activation='relu'))\n",
        "model.add(Dense(2,activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',\n",
        "metrics=['accuracy'])\n",
        "\n",
        "model.fit(train_record_fname,test_record_fname,epochs=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaI8vP26DzZW",
        "colab_type": "text"
      },
      "source": [
        "# Neuer Ansatz A)\n",
        "Eine andere Herangehensweise mit anderen Funktionen und Befehlen. Könnte sich rentieren das Netz in diese Richtung auszubauen bzw Betandteile zu übernehmen.\n",
        "\n",
        "---wurde noch nicht auf das bestehende Netz angepasst---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEZpmU0MNhxw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### A) Neuer Ansatz CNN_model https://androidkt.com/feeding-your-own-data-set-into-the-cnn-model-in-tensorflow/\n",
        "labels=label_map_pbtxt_fname\n",
        "\n",
        "\n",
        "\n",
        "_DEFAULT_IMAGE_SIZE = 252\n",
        "_NUM_CHANNELS = 3\n",
        "_NUM_CLASSES = 4\n",
        "\"\"\"Model function for CNN.\"\"\"\n",
        "def cnn_model_fn(features, labels, mode):\n",
        "    # Input Layer\n",
        "    input_layer = tf.reshape(features[\"image\"], [-1, _DEFAULT_IMAGE_SIZE, _DEFAULT_IMAGE_SIZE, 3])\n",
        "    # Convolutional Layer #1\n",
        "    conv1 = tf.layers.conv2d(\n",
        "        inputs=input_layer,\n",
        "        filters=32,\n",
        "        kernel_size=[5, 5],\n",
        "        padding=\"same\",\n",
        "        activation=tf.nn.relu)\n",
        "    # Pooling Layer #1\n",
        "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
        "    # Convolutional Layer #2 and Pooling Layer #2\n",
        "    conv2 = tf.layers.conv2d(\n",
        "        inputs=pool1,\n",
        "        filters=64,\n",
        "        kernel_size=[5, 5],\n",
        "        padding=\"same\",\n",
        "        activation=tf.nn.relu)\n",
        "    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
        "    # Dense Layer\n",
        "    pool2_flat = tf.reshape(pool2, [-1, 126 * 126 * 64])\n",
        "    dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
        "    dropout = tf.layers.dropout(\n",
        "        inputs=dense, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
        "    # Logits Layer\n",
        "    logits = tf.layers.dense(inputs=dropout, units=2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1zu0mUfPY4Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### A) Neuer Ansatz  Generate Predictions\n",
        "def cnn_model_fn(features, labels, mode):\n",
        "    predictions = {\n",
        "        # Generate predictions (for PREDICT and EVAL mode)\n",
        "        \"classes\": tf.argmax(input=logits, axis=1),\n",
        "        # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n",
        "        # `logging_hook`.\n",
        "        \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
        "    }\n",
        "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
        "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
        "\n",
        "#Calculate Loss\n",
        "#Calculate Loss (for both TRAIN and EVAL modes)\n",
        "onehot_labels = tf.one_hot(indices=tf.cast(labels, tf.int32), depth=2)\n",
        "loss = tf.losses.softmax_cross_entropy(onehot_labels=onehot_labels, logits=logits)\n",
        "\n",
        "\n",
        "\n",
        "#Training Operation\n",
        "if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
        "    train_op = optimizer.minimize(\n",
        "            loss=loss,\n",
        "            global_step=tf.train.get_global_step())\n",
        "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Add evaluation metrics\n",
        "eval_metric_ops = {\n",
        "        \"accuracy\": tf.metrics.accuracy(\n",
        "            labels=labels, predictions=predictions[\"classes\"])}\n",
        "return tf.estimator.EstimatorSpec(\n",
        "        mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mu0dswz2RL3l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### A) Neuer Ansatz Benutzen der trainingsdaten bzw load them\n",
        "def parse_record(raw_record, is_training):\n",
        "    \"\"\"Parse an ImageNet record from `value`.\"\"\"\n",
        "    keys_to_features = {\n",
        "        'image/encoded':\n",
        "            tf.FixedLenFeature((), tf.string, default_value=''),\n",
        "        'image/format':\n",
        "            tf.FixedLenFeature((), tf.string, default_value='jpeg'),\n",
        "        'image/class/label':\n",
        "            tf.FixedLenFeature([], dtype=tf.int64, default_value=-1),\n",
        "        'image/class/text':\n",
        "            tf.FixedLenFeature([], dtype=tf.string, default_value=''),\n",
        "    }\n",
        "    parsed = tf.parse_single_example(raw_record, keys_to_features)\n",
        "    image = tf.image.decode_image(\n",
        "        tf.reshape(parsed['image/encoded'], shape=[]),\n",
        "        _NUM_CHANNELS)\n",
        "    # Note that tf.image.convert_image_dtype scales the image data to [0, 1).\n",
        "    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
        "    image = vgg_preprocessing.preprocess_image(\n",
        "        image=image,\n",
        "        output_height=_DEFAULT_IMAGE_SIZE,\n",
        "        output_width=_DEFAULT_IMAGE_SIZE,\n",
        "        is_training=is_training)\n",
        "    label = tf.cast(\n",
        "        tf.reshape(parsed['image/class/label'], shape=[]),\n",
        "        dtype=tf.int32)\n",
        "    return {\"image\": image}, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdYUNJSdxef6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### A) Neuen Ansatz testen\n",
        "\n",
        "###https://androidkt.com/feeding-your-own-data-set-into-the-cnn-model-in-tensorflow/\n",
        "\n",
        "\n",
        "##Input functions\n",
        "def input_fn(is_training, filenames, batch_size, num_epochs=1, num_parallel_calls=1):\n",
        "    dataset = tf.data.TFRecordDataset(filenames)\n",
        "    if is_training:\n",
        "        dataset = dataset.shuffle(buffer_size=1500)\n",
        "    dataset = dataset.map(lambda value: parse_record(value, is_training),\n",
        "                          num_parallel_calls=num_parallel_calls)\n",
        "    dataset = dataset.shuffle(buffer_size=10000)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.repeat(num_epochs)\n",
        "    iterator = dataset.make_one_shot_iterator()\n",
        "    features, labels = iterator.get_next()\n",
        "    return features, labels\n",
        "def train_input_fn(file_path):\n",
        "    return input_fn(True, file_path, 100, None, 10)\n",
        "def validation_input_fn(file_path):\n",
        "    return input_fn(False, file_path, 50, 1, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23LoqmP_Myom",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### A) Neuer Ansatz Training\n",
        "\n",
        "#Create Estimator\n",
        "classifier = tf.estimator.Estimator(model_fn=cnn_model_fn, model_dir=\"/tmp/convnet_model\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWr-6K6sO3_3",
        "colab_type": "text"
      },
      "source": [
        "# Ansatz mit Transferlearning\n",
        "Dieser Ansatz sollte sicher funktionieren, allerdings wird hierfür ein bereits trainiertes veröffentlichtes CNN benutz. Eine gute Anleitung findet man hier: https://towardsdatascience.com/deeppicar-part-6-963334b2abe0\n",
        "\n",
        "Dies war allerdings nicht Bestandteil des Kurses, da wir den Aufbau und die Funktionsweise des neuronalen Netzes näherbringen wollten. Dies wäre beim Transferlearning nicht möglich gewesen, da die Hauptbestandteile eines fremden bestehenden Netzes einfach benutzt werden würde. Dieses wurde auch schon teilweise über mehrere Wochen durchgehend trainiert.\n",
        "\n",
        "**-->** Die beste Möglichkeit wenn der lehrende Aspekt außer Acht gelassen werden würde\n",
        "\n",
        "Wichtig: das CNN Modell muss allerdings \"quantized\" sein um Daten im TFRecordformat verarbeiten zu können"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lj_zC_OCxygZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model configs are from Model Zoo github: \n",
        "# https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md#coco-trained-models\n",
        "\n",
        "### Empfohlenes Model für die TPU \n",
        "    #http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03.tar.gz\n",
        "   'ssd_mobilenet_v2_quantized': {\n",
        "       'model_name': 'ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03',\n",
        "       'pipeline_file': 'ssd_mobilenet_v2_quantized_300x300_coco.config',\n",
        "       'batch_size': 12\n",
        "    },\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Pick the model you want to use\n",
        "# Select a model in `MODELS_CONFIG`.\n",
        "# Note: for Edge TPU, you have to:\n",
        "# 1) start with a pretrained model from model zoo, such as above 4\n",
        "# 2) Must be a quantized model, which reduces the model size significantly\n",
        "\n",
        "\n",
        "# TPU# selected_model = 'ssd_mobilenet_v2_quantized'\n",
        "\n",
        "# Name of the object detection model to use.\n",
        "MODEL = MODELS_CONFIG[selected_model]['model_name']\n",
        "\n",
        "# Name of the pipline file in tensorflow object detection API.\n",
        "pipeline_file = MODELS_CONFIG[selected_model]['pipeline_file']\n",
        "\n",
        "# Training batch size fits in Colabe's Tesla K80 GPU memory for selected model.\n",
        "batch_size = MODELS_CONFIG[selected_model]['batch_size']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "preprocess: shuffle and resize the images to a uniform size\n",
        "def preprocess(image, label):\n",
        "   resized_image = tf.image.resize(image, [640, 480])\n",
        "   return resized_image, label\n",
        "\n",
        "\n",
        "batch_size = 32\n",
        "print('shape of training data before preprocessing: ', train_data)\n",
        "\n",
        "\n",
        "\n",
        "train_data = train_data.map(preprocess) \\\n",
        " .batch(batch_size).prefetch(1)\n",
        "test_data = test_data.map(preprocess) \\\n",
        " .batch(batch_size).prefetch(1)\n",
        "print('shape of training data after preprocessing: ', train_data)\n",
        "print('shape of test data after preprocessing: ', test_data)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_data = train_data.shuffle(1000)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkQmUNFnWWUi",
        "colab_type": "text"
      },
      "source": [
        "# Saving and Recreating the trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtFY9yPHvWaU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Save the whole model\n",
        "model.save('./trained_CNN/Smart_Truck/my_model_tld1.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1MXKLJsOUJu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Recreate whole model\n",
        "new_model=keras.models.load_model('./trained_CNN/Smart_Truck/my_model_tld1.h5')\n",
        "new_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prUteUyOYFmL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Save the weights\n",
        "model.save_weights('./trained_CNN/Smart_Truck/my_weights_tld1.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKWz5r2zYGIB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Restore the weights\n",
        "model=create_model()\n",
        "model.load_weights('./trained_CNN/Smart_Truck/my_weights_tld1.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4wW0YfdZA4v",
        "colab_type": "text"
      },
      "source": [
        "# DOWNLOAD created files\n",
        "In this case downloading the previously created model.\n",
        "\n",
        "Steps for downloading files manually: Anzeigen -> Inhalt -> Dateien (you can also display and download everything generated)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9LgNdNuZOjA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.download('./trained_CNN/Smart_Truck/my_model_tld1.h5')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}