# -*- coding: utf-8 -*-
"""Smart_Systems_Sample_Solution_Chapter_3_fashion_datasets.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1c8Aaostw3upeSdDyi9lTT96RIhgCKRD3
"""

# Commented out IPython magic to ensure Python compatibility.
"""Smart_Systems_Sample_Solution_Chapter_3_fashion_datasets.ipynb"""

__author__ = "Marius Landmann"

"Other sources which helped writing the code: https://github.com/MariusLandmann/SmartSystems_CNN_TrafficLightDetection/blob/master/Sources/Links.docx"

# TensorFlow â‰¥2.0 is required
# %tensorflow_version 2.x
!pip install --upgrade deeplearning2020
!pip install tensorflow_datasets
!pip install --upgrade tensorflow_datasets
!pip install tfds-nightly
import tensorflow as tf
from tensorflow import keras

assert tf.__version__ >= "2.0"

#Needs the provided GPU from Colab -> test if it is used
if not tf.config.list_physical_devices('GPU'):
    print("No GPU was detected. CNNs can be very slow without a GPU.")
    if IS_COLAB:
        print("Go to Runtime > Change runtime and select a GPU hardware accelerator.")


##Import required tools, layer types
import numpy as np
import tensorflow_datasets as tfds
from tensorflow.keras.layers import Dense, Activation, Input, \
  Dropout, Conv2D, MaxPooling2D, Flatten
from tensorflow.keras.models import Model
import matplotlib.pyplot as plt
from scipy.stats import reciprocal
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
import gzip
from deeplearning2020 import helpers 
import os
import tensorflow_datasets as tfds
!pip install h5py pyyaml
from __future__ import absolute_import, division, print_function

"""# Cloning and Pulling the GitHub repository"""

# Commented out IPython magic to ensure Python compatibility.
# Forked repository
repo_url = 'https://github.com/MariusLandmann/SmartSystems_CNN_TrafficLightDetection'


# Clone repository
# %cd /content

repo_dir_path = os.path.abspath(os.path.join('.', os.path.basename(repo_url)))

!git clone {repo_url}
# %cd {repo_dir_path}

print('Pull it')
!git pull

"""# Loading the Dataset
& unzipping it
"""

# Commented out IPython magic to ensure Python compatibility.
###mnist_reader -> needed for loading the dataset (https://www.coursehero.com/file/49430530/util-mnist-readerpy/)
def load_mnist(path, kind='train'):
    import os

    """Load MNIST data from `path`"""
    labels_path = os.path.join(path,
                               '%s-labels-idx1-ubyte.gz'
#                                % kind)
    images_path = os.path.join(path,
                               '%s-images-idx3-ubyte.gz'
#                                % kind)
#Unzip 
    with gzip.open(labels_path, 'rb') as lbpath:
        labels = np.frombuffer(lbpath.read(), dtype=np.uint8,
                               offset=8)

    with gzip.open(images_path, 'rb') as imgpath:
        images = np.frombuffer(imgpath.read(), dtype=np.uint8,
                               offset=16).reshape(len(labels), 784)

    return images, labels

###Loading the data with Python
X_train, y_train = load_mnist('dataset_FASHION', kind='train')
X_test, y_test = load_mnist('dataset_FASHION', kind='t10k')
## X: images; y: labels

print(X_train.shape)
print(X_test.shape)

"""# Preprocessing the Dataset"""

## Reshape dataset
X_train = X_train.reshape((X_train.shape[0], 28, 28, 1))
X_test = X_test.reshape((X_test.shape[0], 28, 28, 1))

print(X_train.shape)
print(X_test.shape)

## Normalize the pixel values
X_train = X_train / 255.0
X_test = X_test / 255.0

print(X_train.shape)
print(X_test.shape)

## Convert number to a vector --> necessary for the chosen models
total_classes = 10
train_vec_labels = keras.utils.to_categorical(y_train, total_classes)
test_vec_labels = keras.utils.to_categorical(y_test, total_classes)


print('train labels', y_train.shape)
print('test labels', y_test.shape)

print('train labels vector', train_vec_labels.shape)
print('test labels vector', test_vec_labels)

"""# Models with Different Activation Functions and Optimizers"""

### Model with tanh

print(X_train.shape)
print(X_test.shape)


model_tanh = keras.Sequential([
    keras.layers.Flatten(input_shape=(28, 28, 1)),
    keras.layers.Dense(128, activation='tanh'),
    keras.layers.Dense(10, activation='tanh')
])

models = [model_tanh]

[
  model.compile(
      optimizer='sgd',
      loss='mean_squared_error',
      metrics=['accuracy']
  ) for model in models
]


def create_model_tanh():

  model_tanh
  return model_tanh
  return models


model=create_model_tanh()
model.summary()



## Training
epochs=20
[
 model.fit(
    X_train,
    train_vec_labels,
    epochs=epochs,
    verbose=True
  ) for model in models
]


## Evaluation with the test data (new pictures)
_, result_tanh = model_tanh.evaluate(X_test, test_vec_labels)

### Model with sgd

model_sgd = keras.Sequential([
    keras.layers.Flatten(input_shape=(28, 28, 1)),
    keras.layers.Dense(128, activation='relu'),
    keras.layers.Dense(10, activation='sigmoid')
])

models = [model_sgd]

[
  model.compile(
      optimizer='sgd',
      loss='mean_squared_error',
      metrics=['accuracy']
  ) for model in models
]

def create_model_sgd():

  model_sgd
  return model_sgd
  return models


model=create_model_sgd()
model.summary()


## Training
epochs=20
[
 model.fit(
    X_train,
    train_vec_labels,
    epochs=epochs,
    verbose=True
  ) for model in models
]


## Evaluation with the test data (new pictures)
_, result_sgd = model_sgd.evaluate(X_test, test_vec_labels)

### Model with Adam
model_adam = keras.Sequential([
    keras.layers.Flatten(input_shape=(28, 28, 1)),
    keras.layers.Dense(128, activation='relu'),
    keras.layers.Dense(10, activation='sigmoid')
])

models = [model_adam]

[
  model.compile(
      optimizer='adam',
      loss='mean_squared_error',
      metrics=['accuracy']
  ) for model in models
]







def create_model_adam():

  model_adam
  return model_adam
  return models


model=create_model_adam()
model.summary()


## Training
epochs=20
[
 model.fit(
    X_train,
    train_vec_labels,
    epochs=epochs,
    verbose=True
  ) for model in models
]

 
## Evaluation with the test data (new pictures)
_, result_adam = model_adam.evaluate(X_test, test_vec_labels)

"""# Saving and Recreating the Trained Model
Actually not necessary for such simple neurol networks, because the training is very fast. But very useful for CNNs with training phases of several hours/ several days.
"""

## Save the whole model
model.save('./trained_CNN/fashion_MNIST/my_model_adam.h5')

## Recreate whole model
new_model=keras.models.load_model('./trained_CNN/fashion_MNIST/my_model_adam.h5')
new_model.summary()

"""# DOWNLOAD created files
In this case downloading the previously created adam model.

Steps for downloading files manually: Anzeigen -> Inhalt -> Dateien (you can also display and download everything generated).
"""

from google.colab import files
files.download('./trained_CNN/fashion_MNIST/my_model_adam.h5')

"""# Unzip compressed data"""

## Unzip CSV data (Zip) and assign them to variables
import zipfile

with zipfile.ZipFile('/content/SmartSystems_CNN_TrafficLightDetection/dataset_FASHION/in_csv/test_compressed_fashion-mnist_test.csv.zip', 'r') as zip_ref:
    zip_ref.extractall('/content/SmartSystems_CNN_TrafficLightDetection/dataset_FASHION/in_csv')

test_csv = '/content/SmartSystems_CNN_TrafficLightDetection/dataset_FASHION/in_csv/fashion-mnist_test.csv'


with zipfile.ZipFile('/content/SmartSystems_CNN_TrafficLightDetection/dataset_FASHION/in_csv/train_compressed_fashion-mnist_train.csv.zip', 'r') as zip_ref:
    zip_ref.extractall('/content/SmartSystems_CNN_TrafficLightDetection/dataset_FASHION/in_csv')

train_csv = '/content/SmartSystems_CNN_TrafficLightDetection/dataset_FASHION/in_csv/fashion-mnist_train.csv'

!cat {test_csv}